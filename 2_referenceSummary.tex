\section{文献综述}

（该选题在国内/外的研究现状及发展动态；阅读文献的范围以及查阅方式等。博士不得少于3000字，硕士不少于2000字。）

\subsection{网络结构分析技术}
网络结构分析技术是在图论的基础上，将网络的结构作为分析的对象，通过特定算法来提取网络的结构特征。
从其发展历史脉络来看，可以大致分为以下三个阶段，分别是：基于谱图理论的分析方法、基于图信号处理理论的分析方法和基于图嵌入的分析方法。

\begin{figure}
    \centering
    \includegraphics[scale=0.15]{网络结构分析.PNG}
    \caption{网络结构分析技术发展脉络}     \label{history}
\end{figure}

\subsubsection{谱图理论}
基于谱图理论\upcite{nosal1970eigenvalues,chung1997spectral}的分析方法一般将网络结构本身作为主要的分析对象，其输入一般为网络的结构，输出为网络的结构特征参数。

谱图理论基于图论的基础来表示一个网络 $G(V,E)$，其中 $V=(v_1, v_2,...v_n)$ 表示网络中所有节点，$E=(...,e_{ij}, ...)$表示网络中所有的连边。
于是网络结构可以表征为一个邻接矩阵 $W=[w_{ij}]$，其元素满足:
\begin{equation}
    w_{ij}= 
    \begin{cases} 
        0& {e_{ij} \notin E}\\ 
        \epsilon & {e_{ij} \in E}
    \end{cases}
\end{equation}
其中，$\epsilon$ 为边的权重，对于无权重图，$\epsilon=1$。

每个节点的度定义为：
\begin{equation}
    d_i = \sum\limits_{j=1}^{n}w_{ij}
\end{equation}
利用每个节点的度，可以定义一个度矩阵 $D^{n\times n}$：
\begin{equation}
    D = diag(d_1, d_2, ..., d_i,..., d_n)
\end{equation}


通过网络的邻接矩阵和度矩阵，可以定义网络的拉普拉斯矩阵（Graph Laplacian）：
\begin{equation}
    L= D-W
\end{equation}

拉普拉斯矩阵是谱图理论中最为重要的一个分析工具，又被称为图的谱。特别是对于无向图，其拉普拉斯矩阵具有一些很好的性质：
\begin{itemize}
    \item 1.对称性，即所有的特征值均为实数；
    \item 2.二次型满足：$f^\top Lf = \frac{1}{2}\sum\limits_{i,j=1}^{n}w_{ij}(f_i-f_j)^2$；
    \item 3.半正定，即有 $0 =\lambda_1 \leq \lambda_2 \leq... \leq \lambda_n \leq 2\Delta, \quad \Delta=\max(...,d_i,...)$。
\end{itemize}

谱图理论的一个典型应用是谱聚类\upcite{shi2000normalized,ng2001spectral}，谱聚类是一个无向图切图问题，试图找到一个图的最优分割 $A_1,A_2,..A_k$，使得如下目标函数最小：
\begin{equation}
    NCut(A_1,A_2,...,A_k) = \frac{1}{2}\sum\limits_{i=1}^{k}\frac{cut(A_i, \overline{A}_i )}{vol(A_i)}
\end{equation}
其中，$cut(A, B) = \sum\limits_{i \in A, j \in B}w_{ij}$，$vol(A): = \sum\limits_{i \in A}d_i$。从而使分割后同一子类中的连边权重尽可能的大而子类间的连边尽可能的小。
为了便于优化上述目标，可以引入$k$个$n$维的指示向量 $h_j \in \{h_1, h_2,..h_k\}\; j =1,2,...k$ 来表示分割结果，其分量为：
\begin{equation}
    h_{ij}= \begin{cases} 0& { v_i \notin A_j}\\ \frac{1}{\sqrt{vol(A_j)}}& { v_i \in A_j} \end{cases}
\end{equation}
于是有：
\begin{equation}
    \begin{aligned} 
        h_i^{\top}Lh_i & = \frac{1}{2}\sum\limits_{m=1}\sum\limits_{n=1}w_{mn}(h_{im}-h_{in})^2 \\
        & =\frac{1}{2}\Big(\sum\limits_{m \in A_i, n \notin A_i}w_{mn}(\frac{1}{\sqrt{vol(A_i)}} - 0)^2 +  \sum\limits_{m \notin A_i, n \in A_i}w_{mn}(0 - \frac{1}{\sqrt{vol(A_i)}} )^2\Big)\\
        & = \frac{1}{2}\Big(\sum\limits_{m \in A_i, n \notin A_i}w_{mn}\frac{1}{vol(A_i)} +  \sum\limits_{m \notin A_i, n \in A_i}w_{mn}\frac{1}{vol(A_i)}\Big)\\
        & = \frac{1}{2}\Big(cut(A_i, \overline{A}_i) \frac{1}{vol(A_i)} + cut(\overline{A}_i, A_i) \frac{1}{vol(A_i)}\Big) \\
        & =  \frac{cut(A_i, \overline{A}_i)}{vol(A_i)} 
    \end{aligned}
\end{equation}
此时优化的目标可以写为：
\begin{equation}
    NCut(A_1,A_2,...A_k) = \sum\limits_{i=1}^{k}h_i^{\top}Lh_i = \sum\limits_{i=1}^{k}(H^{\top}LH)_{ii} = tr(H^{\top}LH)
\end{equation}
又因为 $H$ 满足 $H^{\top}DH = I$，因此令 $H = D^{-1/2}F$ 可得 $F^{\top}F = I$，此时优化目标进一步改写为：
\begin{equation}
    \underbrace{arg\;min}_F\; tr(F^{\top}D^{-1/2}LD^{-1/2}F) \;\; s.t.\;F^{\top}F=I
\end{equation}
其中，$F$是一组正交基，$D^{-1/2}LD^{-1/2}$ 仍然是对称矩阵，常被记为对称归一化的拉普拉斯矩阵 $\tilde{L}$。
因此，借鉴维度规约的思想，可以通过找到 $\tilde{L}$ 的最小的 $k$ 个特征值对应的特征向量并归一化来作为 $F$ 的近似最优解。
然而，由于使用维度规约损失额少量信息以及正交化变换，$F$不能直接指示各节点的子类归属，因此需要再对齐进行一次传统的聚类（如K-means），即可得到谱聚类的结果。
谱聚类的具体过程如算法\ref{alg:spectral_clustering}所示。
对$F$的传统聚类，就相当于对网络结构在谱空间上的聚类，谱空间的基即为$\tilde{L}$。

\begin{algorithm}[h]
    \caption{谱聚类算法\upcite{ng2001spectral}}
    \label{alg:spectral_clustering}
    \begin{algorithmic}[1] % 1表示显示行号
      \REQUIRE 
        数据集 $X = \{x_1, x_2, ..., x_n\} \subseteq \mathbb{R}^d$（$n$ 个样本，维度 $d$）,
        聚类数 $k$,
        相似性矩阵构建方式（如 $\epsilon$-近邻/全连接）,
        核函数参数（如高斯核带宽 $\sigma$，若使用核方法）
      \ENSURE 
        聚类结果 $\mathcal{C} = \{C_1, C_2, ..., C_k\}$（$C_i$ 为第 $i$ 类的样本索引集合）
      
      \STATE 步骤1：构建相似性矩阵 $W$
      \STATE 对任意样本对 $(x_i, x_j)$，计算相似性权重 $W_{ij}$：
      \STATE \quad 若使用高斯核（RBF）：$W_{ij} = \exp\left(-\frac{\|x_i - x_j\|^2}{2\sigma^2}\right)$（$i \neq j$），$W_{ii} = 0$；
      \STATE \quad 若使用 $\epsilon$-近邻：$W_{ij} = 1$（若 $\|x_i - x_j\| < \epsilon$ 且 $i \neq j$），否则 $W_{ij} = 0$；
      \STATE \quad 保证 $W$ 对称：$W_{ij} = W_{ji}$。
  
      \STATE 步骤2：计算度矩阵 $D$ 和归一化拉普拉斯矩阵 $\tilde{L}$
      \STATE 度矩阵 $D$ 为对角矩阵：$D_{ii} = \sum_{j=1}^n W_{ij}$；
      \STATE 对称归一化拉普拉斯矩阵：$\tilde{L} = I - D^{-1/2} W D^{-1/2}$（$I$ 为单位矩阵）。
  
      \STATE 步骤3：特征分解与降维
      \STATE 对 $\tilde{L}$ 做特征分解，得到特征值 $\lambda_0 \leq \lambda_1 \leq ... \leq \lambda_{n-1}$ 和对应特征向量 $u_0, u_1, ..., u_{n-1}$；
      \STATE 选取前 $k$ 个最小特征值对应的特征向量 $u_0, u_1, ..., u_{k-1}$，构造矩阵 $U \in \mathbb{R}^{n \times k}$（每行对应一个样本，每列对应一个特征向量）；
      \STATE 对 $U$ 的行做归一化（每行除以其 $L_2$ 范数），得到矩阵 $Y \in \mathbb{R}^{n \times k}$：
      \STATE \quad $Y_{ij} = \frac{U_{ij}}{\sqrt{\sum_{l=1}^k U_{il}^2}}$。
  
      \STATE 步骤4：聚类与结果输出
      \STATE 将 $Y$ 的每行视为 $\mathbb{R}^k$ 空间中的一个样本，使用 $k$-means 算法聚类，得到聚类标签 $\{y_1, y_2, ..., y_n\}$（$y_i \in \{1,2,...,k\}$）；
      \STATE 按标签划分聚类：$C_i = \{p \in \{1,..,n\} \mid y_p = i\}$，输出 $\mathcal{C} = \{C_1, ..., C_k\}$。
    \end{algorithmic}
  \end{algorithm}

\subsubsection{图信号处理}
基于图信号处理的分析方法则不仅仅关注网络结构本身，而是考虑将传统的信号分析处理技术延伸到网络这样一种通用的不规则空间中来。这类方法的输入一般为呈现出一定网络结构的信号点的集合，输出为信号特征。

由于许多传统的信号处理方法都是针对规则的欧几里得空间上定义的数据而设计的。
然而，在一些特定应用场景中，涉及的数据定义在拓扑结构更复杂的空间中，例如，计算机网络、交通（公路、铁路、飞机）网络或社交网络都可以用加权图来描述，顶点分别对应于单个计算机、城市或人。
一些经典的信号处理技术往往需要在处理前对信号进行变换，例如傅里叶变换和小波变换，从而在变换后的空间中来完成特征提取。
基于小波变换的方法的强大之处在于它们能够同时在时域和频域上对信号内容进行局部化。
对于主要信息内容存在于局部奇异点的信号，如时间序列信号中的阶跃不连续或图像中的边缘，小波可以提供比原始域或具有全局基元的变换更紧凑的表示。
小波变换在规则空间上对信号处理问题的有效性激发了对不规则的非欧氏空间的扩展研究。

经典小波是通过平移和缩放单个母波来构造的，转换系数由输入函数与这些转换和缩放波形的内积给出。
由于不清楚如何在不规则图上定义缩放和平移，因此难以直接将这种结构扩展到任意加权图。
Hammond等人\upcite{hammond2011wavelets}利用谱图理论的相关工作来解决这个问题，即以图拉普拉斯矩阵作为变换后的特征空间，类似于图上的傅里叶变换。

对于任意定义在节点上的的函数$f\in \mathbb{R}^N$，其图傅里叶变换为：
\begin{equation}
    \hat f(\mathcal{l}) = \langle \mathcal{X_l}, f\rangle = \sum_{n=1}^{N} \overline{\mathcal{X_l}(n)}f(n)
\end{equation}
对应的图傅里叶逆变换为：
\begin{equation}
    f(n)=\sum_{\mathcal{l}=0}^{N-1} \hat f(\mathcal{l})\mathcal{X_l}(n)
\end{equation}
显然，图傅里叶域以拉普拉斯矩阵的特征向量为基。图傅里叶变换的矩阵形式为：
\begin{equation}
    \begin{aligned}
        \hat x=U^\top x \\
        x=U\hat x 
    \end{aligned}
\end{equation}

图傅里叶变换的提出解决了不规则图上平移和缩放操作的尺度不变性问题，而如何基于图傅里叶变化来设计高效的局部滤波器则是图信号处理中的另一个关键问题。
卷积神经网络在图像处理领域取得了非常好的效果，通过定义网络结构上的卷积核来处理图信号也随之成为一个热门的研究方向。
Defferrard等人\upcite{defferrard2016convolutional} 在图傅里叶变换的基础上，进一步定义了图卷积操作，并通过对卷积核函数采用特定的参数化方式保证了其作用范围的局部性。
首先，借助图傅里叶变换，图上的卷积算子被定义如下：
\begin{equation}
    x \ast y = U((U^\top x)\odot (U^\top y))
\end{equation}
其中，$x,y $是定义在图上的离散函数。因此可以在图上定义对信号$x$的卷积操作：
\begin{equation}
    x'=Ug_\theta (\Lambda)U^\top x
\end{equation}
其中，$g_\theta (\Lambda)$表示变换到傅里叶域中的卷积核，而$\Lambda$代表了图的一些结构性质。对于一个完全自由的卷积核，其可以完全独立于图的结构而被定义如下：
\begin{equation}
    g_\theta(\Lambda)=\text{diag}(\theta)\quad \theta \in \mathbb{R}^N
\end{equation}
即$\theta $作为傅里叶域基底的系数。然而这样的定义存在两个问题：一是不能保证卷积的操作的局部性，二是参数学习的成本太高。针对这两个问题，可以定义多项式卷积核：
\begin{equation}
    g_\theta(\Lambda)=\sum_{k=0}^{K-1}\theta_k\Lambda^k
\end{equation}
即$\theta $作为关于$\Lambda$的多项式的系数。此时有：
\begin{equation}
    \begin{aligned}
        x'&=Ug_\theta (\Lambda)U^\top x \\
        &=U\sum_{k=0}^{K-1}\theta_k\Lambda^kU^\top x \\
        &=\sum_{k=0}^{K-1}\theta_kU\Lambda^kU^\top x \\
        &=\sum_{k=0}^{K-1}\theta_kL^k x
    \end{aligned} 
\end{equation}
可以 Kronecker delta 函数$\delta_i$作为卷积的输入来分析节点$i$经过卷积后的效果：
\begin{equation}
    \begin{aligned}
        \delta_i'&=\sum_{k=0}^{K-1}\theta_kL^k \delta_i \\
        &=\sum_{k=0}^{K-1}\theta_kL_{i}^k
    \end{aligned} 
\end{equation}

不难发现，这里矩阵$L$类似于一步转移矩阵，因此当节点$i,j$之间的最短路径长度大于$K $时$\delta_{ij}'=0$。因此，这种卷积操作能够确保局部性为$K $，同时，参数学习的成本也大大降低了：$K\ll N$。然而，卷积操作$x'=Ug_\theta (\Lambda)U^\top x$的代价依然很高，因为涉及和傅里叶基的矩阵乘法，所以复杂度是$O(N^2)$。一种解决思路是将其参数化为能够通过对$L$迭代计算得到的多项式，即进行$K$次稀疏的矩阵乘法来降低计算复杂度：$O(K|\varepsilon|)\ll O(N^2)$。一种满足条件的多项式，是切比雪夫多项式（Chebyshev expansion），通常在图信号处理中用来近似小波：
\begin{equation}
    \begin{aligned}
        T_k(x)=
          \begin{cases}
          2xT_{k-1}(x)-T_{k-2}(x), \quad k>1\\
          x,\quad k=1\\
          1, \quad k=0
          \end{cases}
    \end{aligned} 
\end{equation}
首先，这些多项式在希尔伯特空间上构成一组正交基：
\begin{equation}
    \int_{-1}^1 T_m(x) \cdot T_n(x) \cdot \frac{1}{\sqrt{1-x^2}} dx = 
    \begin{cases} 
    0 & (m \neq n), \\
    \pi & (m = n = 0), \\
    \pi/2 & (m = n \geq 1).
    \end{cases}
\end{equation}
正交区间为$[-1,1]$，能够在此区间上逼近任意的平方可积函数。于是参数化定义傅里叶域中的卷积核：
\begin{equation}
    g_\theta(\Lambda)=\sum_{k=0}^{K-1}\theta_kT_k(\tilde{\Lambda})
\end{equation}
因为$\tilde{\Lambda}=2\Lambda/\lambda_{max}-I_n$将特征值缩放到切比雪夫多项式的正交区间，所以这种参数化方法的表示能力等价于前一种方式。于是，新参数化方法下的卷积操作变为：
\begin{equation}
    \begin{aligned}
        x'&=U\sum_{k=0}^{K-1}\theta_kT_k(\tilde{\Lambda})U^\top x \\
        &=\sum_{k=0}^{K-1}\theta_kT_k(\tilde{L}) x \\
    \end{aligned} 
\end{equation}
其中$\tilde{L}=2L/\lambda_{max}-I_n$，$\lambda_{max}=2\Delta$，$\Delta$为最大度。

为了能够使图卷积操作能够更好的适配深度学习框架，Kipf等人\upcite{kipf2017semi}在切比雪夫多项式参数化图卷积核的基础上提出了基于局部一阶近似的图卷积操作，图卷积层按如下定义：
\begin{equation}
    H^{(l+1)}=\sigma\Big( \tilde{D}^{-\frac{1}{2}}\tilde{A}\tilde{D}^{-\frac{1}{2}}H^{(l)}W^{(l)} \Big)
\end{equation}
其中，$\tilde{A}=A+I_n$表示加入了自连接的无向图的邻接矩阵。$\tilde{D}_{ii}=\sum_j\tilde{A}_{ij}$表示$\tilde{A}$的度矩阵。
当限制GCN的局部感受野$K=1$时，卷积操作就变成了关于$L$的线性函数:
\begin{equation}
    \begin{aligned}
        x'=g_{\theta}\ast x\approx \theta_0x+\theta_1\tilde{L}
        x
        &=\theta_0x+\theta_1D^{-\frac{1}{2}}LD^{-\frac{1}{2}}x \\
        &=\theta_0x+\theta_1D^{-\frac{1}{2}}(D-A)D^{-\frac{1}{2}}x \\
        &=\theta_0x+\theta_1(I_N-D^{-\frac{1}{2}}AD^{-\frac{1}{2}})x \\
        &=(\theta_0+\theta_1)x-\theta_1D^{-\frac{1}{2}}AD^{-\frac{1}{2}}x \\
        &=\theta_0x-\theta_1D^{-\frac{1}{2}}AD^{-\frac{1}{2}}x \\
    \end{aligned} 
\end{equation}
进一步简化参数，令$\theta=\theta_0=-\theta_1$，于是：
\begin{equation}
    \begin{aligned}
        x'=g_{\theta}\ast x\approx \theta(I_N+D^{-\frac{1}{2}}AD^{-\frac{1}{2}})x
    \end{aligned}
\end{equation}
$I_N+D^{-\frac{1}{2}}AD^{-\frac{1}{2}}$特征值的范围是$[0,2]$，在深度学习过程中重复应用这个卷积算子可能会导致数值不稳定和梯度弥散或爆炸的问题，为了避免这个问题，再引入一个被称为重归一化的技巧：
\begin{equation}
    I_N+D^{-\frac{1}{2}}AD^{-\frac{1}{2}}\rightarrow \tilde{D}^{-\frac{1}{2}}\tilde{A}\tilde{D}^{-\frac{1}{2}}
\end{equation}
其中，$\tilde{A}=A+I_N$，$\tilde{D}_{ii}=\sum_j\tilde{A}_{ij}$，于是对于具有$C$个通道的图信号$X\in \mathbb{R}^{N\times C}$，图卷积过程定义如下：
\begin{equation}
    X'=\tilde{D}^{-\frac{1}{2}}\tilde{A}\tilde{D}^{-\frac{1}{2}}X\Theta
\end{equation}
其中，$\Theta\in \mathbb{R}^{C\times F}$是全部的卷积核参数（$C$个通道，每个通道$F$个卷积核）。
这种图卷积层大大降低了计算复杂度，并且适配端到端的训练框架，能够同时处理节点特征和图结构信息。
通过高效的矩阵运算，同时利用少量标签数据和大量无标签数据进行训练，显著提高分类性能，使模型能够处理大规模图数据，并在Cora、CiteSeer、PubMed数据集上进行了验证。
实验结果表明，该方法在半监督分类任务上取得了显著优于传统方法的性能，特别是在标签数据有限的情况下表现尤为突出。

\subsubsection{图嵌入}
基于图嵌入的分析方法在图信号处理的基础上，借鉴深度学习相关技术的成果和经验，将表示学习的思路应用于网络结构特征的提取。这类方法的输入仍然是一堆具有网络结构的信号点，但输出除了信号特征外，还包括学习到的特征提取器和节点的嵌入向量表示。

图嵌入方法的核心是图神经网络（Graph Neural Network，GNN），即使用神经网络将图的结构信息编码到节点的表示中去。
早期的图神经网络主要是循环图神经网络，后来随着图信号处理技术和硬件技术的发展又出现了卷积图神经网络和自编码图神经网络。

\paragraph{循环图神经网络}
循环图神经网络算法的思想是基于信息扩散机制，模拟网络节点之间相互影响的过程，直至达到一个均衡的状态，从而实现将网络结构信息编码进节点表示的目的。具体实现过程是，用一组同样的参数来迭代的更新每个节点的状态表示，直至收敛：
\begin{equation}
    h_v^{(t)}=\sum_{u\in N(v)}f(x_v,x_{(v,u)}^e,x_u,h_u^{(t-1)})
\end{equation}
其中，$h_v^{(t)}$是节点$v$在$t$时刻的状态，$f(\cdot)$是参数化的函数。
其输入包括：节点$v $的属性特征$x_v$、与节点$v$相连的边的属性特征$x_{(v,u)}^e$、节点$v$的邻居的属性特征$x_u$以及节点$v$的邻居上一时刻的状态$h_u^{(t-1)}$。
可见，循环图神经网络算法中的网络层类似于社群分类中常用的局部条件分类器\upcite{sen2008collective}，是一种依靠局部更新实现自举（bootstrapping）的方法，不过前者的输出是节点的表示，后者输出的是类别标签。
为了保证收敛性，通常需要限制函数$f(\cdot)$为一种收缩映射（contraction mapping）。

Sperduti等人\upcite{sperduti1997supervised}最开始使用有监督学习的神经网络来解决网络结构的分类问题，使用的就是循环迭代的思路，但受限于当时计算机的性能，能够处理的网络结构被限定为有向无环图，而且该项工作也普遍被认为是最早的图神经网络的研究之一\upcite{velickovic2017graph,wu2020comprehensive}。
Scarselli等人\upcite{scarselli2008graph}正式提出了图神经网络的概念，并在之前工作的基础上扩展了可处理的网络结构。
在该模型中，包含两个部分：负责编码节点表示的多层感知机网络$f$和负责对$f$输出的表示进行分类的判别器$g$。
在训练过程中，正向迭代获取节点表示和反向迭代通过梯度更新$f$的参数交替进行以优化训练目标，这种训练策略使得该模型能够应对网络中的环状结构。
Gallicchio等人\upcite{gallicchio2010graph}在此基础上引入了一个虚拟的回声节点（Echo）来加速迭代过程，此外，该方法选择训练输出层而非编码层，从而避免了迭代的编码过程给训练带来的巨大计算开销。
Li等人\upcite{li2015gated}使用了当时更为先进的门控循环单元（Gated Recurrent Unit，GRU\upcite{cho2014learning}）代替传统的多层感知机结构作为编码节点表示的网络模块，进一步解决了训练过程中的长时依赖和梯度弥散问题，并且不需要再限制参数以保证收敛性，从而提升了模型表示能力。
然而当处理的网络规模进一步加大时，循环神经网络的长时依赖问题依然突出，并且会因为要存储节点的中间状态而面临极大的存储开销。
对此，Dai等人\upcite{dai2018learning}提出了一种适应大规模网络的节点表示学习算法，该方法每次迭代随机采样一个批次的节点进行状态更新，采样另一个批次的节点进行梯度更新，从而有效限制了每次迭代的计算量。
同时，为了保证计算的稳定性，该方法采用了一种类似Gibbs采样和动量梯度的策略来定义循环函数。

\paragraph{卷积图神经网络}
卷积图神经网络算法的思想是将图像、声音信号处理领域中的卷积操作扩展到图这样的非欧几何领域中来，从而实现利用图卷积操作来进行局部特征提取的目的。
卷积图神经网络与循环图神经网络虽然都是从网络局部特征入手来编码网络结构，但卷积网络仅需固定的编码层数且不同层之间参数不同，因此在编码效率和表示能力上较循环网络有巨大优势。\upcite{wu2020comprehensive}

卷积图神经网络又可以分为基于频域（Spectral-based）的方法和基于空域（Spatial-based）的方法。
其中，前文提到的Kipf等人\upcite{kipf2017semi}的工作就是基于频域方法的重要代表。
这类方法延续了谱图理论和图信号处理相关工作的研究思路，始终围绕网络结构的谱来构造合适的局部滤波器。
例如，Levie等人\upcite{levie2018cayleynets}选择用凯利多项式来进行卷积核的参数化，从而利用凯利多项式作为有理复函数的特性来捕获窄频带。
而且，切比雪夫多项式参数化可以看作是凯利多项式参数化的一个特例。
Li等人\upcite{li2018adaptive}提出了一种自适应图卷积神经网络，通过学习任务驱动的自适应图结构，采用距离度量学习方法，使图卷积神经网络能够适应不同大小和连接性的图结构数据，并且提高了收敛速度和预测准确性。
Zhuang等人\upcite{zhuang2018dual}提出了一种适用于图结构数据的简单且可扩展的半监督学习方法，通过双图卷积网络同时考虑局部一致性和全局一致性，创新性地将未标记数据的图知识充分嵌入，实现了在少量标记数据情况下的高效学习。

基于空域的方法则绕开了网络结构谱这样一个可解释的概念，从而像图像卷积那样直接在空间的邻接关系上定义卷积核，将中心节点的表示与相邻节点的表示进行卷积，从而更新中心节点的表示。
其操作本质上是沿边缘传播节点信息，这与循环网络的思想十分类似。
在图傅里叶变换出现之前，Micheli\upcite{micheli2009neural}就提出了一种新的针对网络结构的学习方法——NN4G，算是最早出现的图卷积网络。
其通过构造性反馈前馈架构实现适应性上下文转导，扩展了神经网络在图结构上的应用范围。
与循环网络不同的是，NN4G通过每层具有独立参数的组合神经结构来学习图的相互依赖性。节点的邻域可以通过体系结构的增量构建来扩展。
受NN4G启发，Bacciu等人\upcite{bacciu2018contextual}提出了一种结合生成模型和神经网络的新方法——上下文图马尔可夫模型，通过构建深度概率模型层，实现对图结构信息的有效编码和分类。
Atwood等人\upcite{atwood2016diffusion}等人将卷积操作视为一个信息扩散的过程，假设信息以一定的转移概率从一个节点传递到它的相邻节点，使信息分布经过几轮后达到均衡。
其将扩散的图卷积定义为：
\begin{equation}
    H^{(k)}=f(W^{(k)}\odot P^{(k)}X)
\end{equation}
其中，$f$是激活函数，转移概率矩阵$P=D^{-1}A$。从定义可以看出，每经过一次扩散图卷积，就相当于信息进行了一步概率转移。
后续一些经典基于空域的图卷积方法大都延续了这一思路。\upcite{li2017diffusion,tran2018filter,yan2018spatial,gilmer2017neural}

相比于基于频域的方法，基于空域的方法因为不依赖网络结构谱，从而能在网络结构未明确的情况下保持兼容性。
例如，William等人\upcite{hamilton2017inductive}就针对传统直推式（Transductive）表示学习方法在网络结构发生变化时泛化性差的问题，提出了GraphSAGE。
其通过采样和聚合节点局部邻域的特征来归纳式（Inductive）的学习生成节点表示的函数，而不是为每个节点单独训练表示。
GraphSAGE作为一个通用的归纳框架，利用节点特征信息来有效生成先前未见数据的节点表示，在归纳式表示学习任务中优于当时的其他方法。

\paragraph{自编码图神经网络}
自编码图神经网络算法的思想是代理任务驱动，借助神经网络的表示学习能力和弱监督信号的引导，从而在训练的过程中顺便获得节点的空间嵌入表示。

例如，Aditya等人\upcite{grover2016node2vec}提出了一种名为Node2Vec的算法框架，借鉴了自然语言处理中Word2Vec的表示学习思路，以一种灵活的节点网络邻域定义和有偏随机游走策略，能够探索多样化的邻域结构。
当邻域结构确定后，对网络结构的表示学习过程即对应于优化以下目标：
\begin{equation}
    \max_{f} \quad \sum_{u\in V} \log \text{Pr}(\text{N}_{\text{S}}(u)|f(u))
\end{equation}
其中，$\text{N}_{\text{S}}(u)$表示节点$u$的邻域，基于邻域的条件独立性和对称性假设有：
\begin{equation}
    \begin{aligned}
        \text{Pr}(\text{N}_{\text{S}}(u)|f(u))= \prod_{n_i \in \text{N}_{\text{S}}(u)} \text{Pr}(n_i|f(u)) \\
        \text{Pr}(n_i|f(u))=\frac{\exp(f(n_i)\cdot f(u))}{\sum_{v\in V}\exp(f(v)\cdot f(u))}
    \end{aligned}
\end{equation}
此时，目标函数可以转化为：
\begin{equation}
    \begin{aligned}
            &\max_{f} \quad \sum_{u\in V} \Big[ \sum_{n_i \in \text{N}_{\text{S}}(u)} \log \text{Pr}(n_i|f(u)) \Big] \\
            \Rightarrow &\max_{f} \quad \sum_{u\in V} \Big[ \sum_{n_i \in \text{N}_{\text{S}}(u)} \log \frac{\exp(f(n_i)\cdot f(u))}{\sum_{v\in V}\exp(f(v)\cdot f(u))} \Big] \\
            \Rightarrow &\max_{f} \quad \sum_{u\in V} \Big[ -\log Z_u + \sum_{n_i \in \text{N}_{\text{S}}(u)} f(n_i)\cdot f(u)) \Big]
    \end{aligned}
\end{equation}
其中，配分项$Z_u=\sum_{v\in V}\exp(f(v)\cdot f(u))$在网络规模很大时会变得难以计算，因此将通过负采样来统一近似估计。

Node2Vec所构造的目标函数实际上是在最大化采样邻域的似然度，这本身就是一种自编码的结构，期望节点的嵌入表示能够最大程度还原网络的结构。
其他的自编码图神经网络算法基本都秉持着这样的思路，只是在实现的技术路线上有所不同。
早期的方法使用多层感知机来学习节点的嵌入表示。Perozzi等人\upcite{perozzi2014deepwalk}提出了一种名为DeepWalk的方法，利用随机游走在线学习网络中顶点的潜在表示，将语言模型和深度学习技术从单词序列推广到图结构，通过截断随机游走获取局部信息，将这些游走视为句子来学习顶点的潜在表示。Cao等人\upcite{cao2016deep}在DeepWalk的基础上改进其线性采样生成序列的方式，基于一组降噪自编码器结构，采用随机冲浪模型直接捕获图结构信息。Wang等人\upcite{wang2016structural}提出了一种名为SDNE的方法，使用半监督深度模型和同时利用一阶和二阶邻近性来有效捕捉和保留网络结构的高度非线性特征，解决了浅层模型无法准确表征复杂网络结构的问题。Kipf等人\upcite{kipf2016variational}提出了变分图自动编码器（VGAE），采用了一种基于变分的图自动编码器框架，使用图卷积网络作为编码器，内积解码器进行重构。

自编码图神经网络的另一个重要作用是进行图生成，即基于学习到的节点嵌入表示通过解码器来生成图结构，这在链路预测、网络重构任务中有着重要的应用价值。
其中，序列化的生成方法\upcite{gomez2018automatic,kusner2017grammar,dai2018syntax}采用逐节点、逐连接的方式依从从局部生成网络的结构；全局化的生成方法\upcite{simonovsky2018graphvae,ma2018constrained,de2018molgan,schlichtkrull2018modeling,gulrajani2017improved}则采取一次性生成全部网络结构的方式，这类方法通常将节点和边的存在性建模为独立的随机变量并基于概率图模型来对网络结构进行推断。

\subsection{时空序列预测}
在大多数实际应用场景中，网络在结构和输入上往往都是动态的，传统的时间序列分析技术难以处理多个序列的异质性和相互依赖关系，而传统的网络结构分析技术则难以处理时间上的动态性。
因此，时空序列分析技术则着眼于解决这类数据挖掘问题。其输入是一组彼此关联且呈现出一定结构的时间序列的集合，同时具有时间和空间上的信息。输出则是时空序列的演化规律以及对未来演化结果的预测。

例如，城市的交通系统就是一个典型的时空序列，城市道路连接不同的场所地点在地理空间上形成了交通网络，而不同地点的交通状况会随着时间而不断变化。
而交通流量预测问题，就是一个十分具有现实意义的研究方向，在智能运输系统和智慧城市建设中有着广泛的应用前景。
交通流量的预测，可以被抽象定义为时空序列预测问题，其输入包括：交通网络$G(V,E)$，以及历史交通信号序列$(X_1, X_2,...,X_T)\in \mathbb{R}^{T\times |V|\times C}$，其中$T$表示在不同时刻的采样点的数量，$C$表示采样特征的通道数量。输出为预测的未来$t$个时刻的交通信号：$(X_{T+1}, X_{T+2},...X_{T+t})\in \mathbb{R}^{t\times |V|\times C}$

\small
\begin{longtable}{|m{0.17\textwidth}|m{0.18\textwidth}|m{0.15\textwidth}|m{0.20\textwidth}|m{0.15\textwidth}|}
    % 表格头部（每页重复）
    \hline
    \centering\fangsong 模型 & 
    \centering\fangsong 空间拓扑构造 & 
    \centering\fangsong 空域特征提取 & 
    \centering\fangsong 时域特征提取 & 
    \centering\fangsong 额外特征 \\
    % \hline
    \endfirsthead  % 第一页头部结束
  
    % 后续页头部
    \hline
    \centering\fangsong 模型 & 
    \centering\fangsong 空间拓扑构造 & 
    \centering\fangsong 空域特征提取 & 
    \centering\fangsong 时域特征提取 & 
    \centering\fangsong 额外特征 \\
    % \hline
    \endhead
  
    % 表格底部（每页重复）
    \hline
    \multicolumn{5}{r}{\fangsong 续表} \\
    \endfoot
  
    % 最后一页底部
    \hline
    \endlastfoot
  
    % 表格内容（仿宋字体，适配模板全局字体）
    \hline
    \fangsong DGCRNN\upcite{li2017diffusion} & \fangsong 距离 & \fangsong GCN & \fangsong GRU & \fangsong Time \\
    \hline
    \fangsong AGC-Seq2Seq\upcite{zhang2018multistep} & \fangsong 联通性 & \fangsong GCN & \fangsong GRU+Attn & \fangsong None \\
    \hline
    \fangsong TGC-LSTM\upcite{cui2019traffic} & \fangsong 联通性+其他 & \fangsong GCN & \fangsong LSTM & \fangsong None \\
    \hline
    \fangsong ST-MGCN\upcite{geng2019spatiotemporal} & \fangsong 联通性+其他 & \fangsong GCN & \fangsong RNN & \fangsong POI+道路网 \\
    \hline
    \fangsong ST-MetaGCN\upcite{pan2019urban} & \fangsong 距离 & \fangsong Meta-GAT & \fangsong Meta-RNN & \fangsong Time+道路网 \\
    \hline
    \fangsong MRA-BNet\upcite{chen2020multi} & \fangsong 距离+其他 & \fangsong GCN+Attn & \fangsong GRU & \fangsong Time \\
    \hline
    \fangsong DGCRN\upcite{guo2020dynamic} & \fangsong 联通性 & \fangsong GCN+Attn & \fangsong LSTM+Attn & \fangsong Time \\
    \hline
    \fangsong AGCN\upcite{bai2020adaptive} & \fangsong 自适应 & \fangsong GCN & \fangsong GRU & \fangsong Time \\
    \hline
    \fangsong STGNN\upcite{wang2020traffic} & \fangsong 自适应+距离 & \fangsong GCN & \fangsong GRU+Transformer & \fangsong Time \\
    \hline
    \fangsong GMAN\upcite{zheng2020gman} & \fangsong 距离 & \fangsong 嵌入+Attn & \fangsong Embedding+Attn & \fangsong Time \\
    \hline
    \fangsong ST-GRAT\upcite{kim2020stgrat} & \fangsong 距离 & \fangsong 嵌入+Attn & \fangsong Embedding+Attn & \fangsong Time \\
    \hline
    \fangsong STTNs\upcite{xu2020spatial} & \fangsong 自适应 & \fangsong GCN & \fangsong CNN & \fangsong Time \\
    \hline
    \fangsong STGCN\upcite{yu2017spatio} & \fangsong 距离 & \fangsong GCN & \fangsong CNN & \fangsong Time \\
    \hline
    \fangsong ASTGCN\upcite{guo2019attention} & \fangsong 联通性 & \fangsong GCN+Attn & \fangsong CNN+Attn & \fangsong Time \\
    \hline
    \fangsong STSGCN\upcite{song2020spatial} & \fangsong 联通性 & \fangsong GCN & \fangsong CNN & \fangsong Time \\
    \hline
    \fangsong LSGCN\upcite{huang2020lsgcn} & \fangsong 联通性 & \fangsong GCN+Attn & \fangsong CNN & \fangsong Time \\
    \hline
    \fangsong Graph WaveNet\upcite{wu2019graph} & \fangsong 自适应+距离 & \fangsong GCN & \fangsong CNN & \fangsong Time \\
    \hline
    \fangsong SLCNN\upcite{zhang2020spatio} & \fangsong 自适应 & \fangsong GCN & \fangsong CNN & \fangsong Time \\
    \hline
    \fangsong MTGNN\upcite{wu2020connecting} & \fangsong 自适应 & \fangsong GCN & \fangsong CNN & \fangsong Time \\
    \hline
    \fangsong ST-ResNet\upcite{zhang2017deep} & \fangsong 网格化 & \fangsong CNN & \fangsong CNN & \fangsong Time+Weather \\
    \hline
    \fangsong DeepSTN+\upcite{lin2019deepstn+} & \fangsong 网格化 & \fangsong CNN & \fangsong CNN & \fangsong Time+POI \\
    \hline
    \fangsong DMVST-Net\upcite{yao2018deep} & \fangsong 网格化+其他 & \fangsong CNN+嵌入 & \fangsong RNN & \fangsong Time+Weather \\
    \hline
    \fangsong STDN\upcite{yao2019revisiting} & \fangsong 网格化 & \fangsong CNN & \fangsong RNN+Attn & \fangsong Time \\
    \hline
    \fangsong DARNN\upcite{qin2017dual} & \fangsong None & \fangsong None & \fangsong RNN+Attn & \fangsong None \\
    \hline
    \fangsong GeoMAN\upcite{liang2018geoman} & \fangsong None & \fangsong Attn & \fangsong RNN+Attn & \fangsong Time+Weather+POI
\end{longtable}
\normalsize

交通流预测的关键挑战在于时间和空间两个维度上的信息相互依赖，因此当前主流的方法大都着眼于寻求好的信息融合方法来提高预测的准确率。
对于时间维度上的信息，使用循环神经网络（如LSTM）来提取特征是一种十分自然的思路\upcite{li2017diffusion,seo2018structured,geng2019spatiotemporal}。然而，循环神经网络存在表示能力不足和并行效率低下的问题，并且循环神经网络自身存在长时依赖问题\upcite{bengio1994learning}，因此也有使用1维卷积网络来提取特征的方法\upcite{yan2018spatial,wu2019graph,guo2019attention,song2020spatial}。
随着以Transformer结构\upcite{vaswani2017Attn}中的多头注意力机制在序列表示学习任务中展现出的优越性能，基于注意力机制的时序特征表示也被逐渐用于时空序列预测的模型管道当中\upcite{guo2019attention,guo2021learning}。

对于空间维度上的信息，在图卷积出现之前，一种通行的做法是按区域将地理位置划分为网格，从而使用卷积神经网络加以处理。\upcite{lin2019deepstn+,zonoozi2018periodic,yao2019revisiting}
然而，这种网格化的方法难以利用交通网络的拓扑结构信息，因此也有直接以交通网络为输入来进行预测的方法\upcite{chen2020multi,song2020spatial,guo2019attention,song2020spatial}。

除了对时空序列的信号进行预测外，还有一些对于系统状态的预测工作。例如，Liu等人\upcite{liu2024deep}研究复杂系统的韧性时，以系统的网络拓扑以及演化的时空序列为输入，利用深度学习的方法来推断系统是否达到稳态、以及是否会偏离稳态。

\subsection{网络重构}
网络重构要解决的问题是，在获取网络结构信息有限的约束下，来推断出更完整的网络结构信息。其输入是除完整网络结构外对网络的一切其他观测信息，输出是推断出的网络结构。
基于信息论和复杂网络熵的度量\upcite{anand2009entropy}，网络重构问题可以被定义为如下带约束的优化问题\upcite{squartini2018reconstruction}：
\begin{equation}
    \mathscr{L}[P] = S - \lambda_0 \left[ \sum_{\mathbf{G} \in \mathcal{G}} P(\mathbf{G}) - 1 \right] - \sum_{m=1}^M \lambda_m \left[ \sum_{\mathbf{G} \in \mathcal{G}} P(\mathbf{G}) C_m(\mathbf{G}) - \langle C_m \rangle \right]
\end{equation}

其中，$\mathcal{G}$是网络系综（Ensemble），$\mathbf{G}$是网络构型（Configuration），$S$是网络构型分布$P(\mathbf{G})$的熵。
$\lambda_0, \lambda_m$分别是拉格朗日乘子，用于引入约束条件，平衡熵与约束的权重。
$C_m(\mathbf{G})$是第$m$个“网络可观测变量”，$\langle C_m \rangle$是$C_m(\mathbf{G})$的观测值。
这个公式的核心思想是在最大化网络各观测变量观测值似然度的约束下，最大化网络构型分布的熵。

指数随机图模型（Exponential Random Graphs，ERG）\upcite{robins2007introduction}在大多数网络重构算法中扮演中十分重要的角色。
其通过最大化$\mathscr{L}[P]$来获得，即令：
\begin{equation}
    \frac{\delta \mathscr{L}[P]}{\delta P(\mathbf{G})} = 0
\end{equation}
从而得到：
\begin{equation}
    P(\mathbf{G} | \vec{\lambda}) = e^{-1 - \lambda_0 - \sum_{m=1}^M \lambda_m C_m(\mathbf{G})}
\end{equation}
这是一个指数形式的分布，故而得名指数随机图。其中的$\vec{\lambda}$是拉格朗日乘数。
考虑到归一化约束$\sum_{\mathbf{G}} P(\mathbf{G}) = 1$，从而能够定义一个配分函数：
\begin{equation}
    e^{1 + \lambda_0} \equiv Z(\vec{\lambda}) = \sum_{\mathbf{G} \in \mathcal{G}} e^{-\sum_{m=1}^M \lambda_m C_m(\mathbf{G})}  
\end{equation}
于是有ERG的标准形式：
\begin{equation}
    P(\mathbf{G} | \vec{\lambda}) = \frac{e^{-\sum_{m=1}^M \lambda_m C_m(\mathbf{G})}}{Z(\vec{\lambda})}
\end{equation}
其中，指数部分的$H(\mathbf{G} | \vec{\lambda})=\sum_{m=1}^M \lambda_m C_m(\mathbf{G})$被称为图汉密尔顿项（graph Hamiltonian）。
基于ERG，可以方便的分析真实世界中的网络构型$\hat{\mathbf{G}}$并且连续一致的给出其概率系数$P(\hat{\mathbf{G}} | \vec{\lambda})$

在大数据时代，研究者往往能获取节点状态的时间序列数据，却难以直接观测节点间的连接结构与动力学规则，这一困境催生了复杂网络重构这一前沿交叉方向。其核心目标是通过数据驱动的算法，从观测数据中逆向推断网络拓扑结构、动力学演化规律及潜在交互机制，为复杂系统的理解、预测与控制提供基础。
Shen等人\upcite{shen2014reconstructing}提出基于压缩感知的传播网络重构方法，将网络结构推断转化为凸优化问题，实现对具有自然多样性的传播网络及隐藏源节点的识别，显著提升了稀疏网络的重构效率。Wang等人\upcite{wang2011time}提出时序数据驱动的压缩感知重构方法，针对振荡器网络的特性，通过压缩感知技术从时间序列中提取网络连接信息，实现对复杂振荡系统的结构预测。Wu等人\upcite{wu2021recovering}通过整合多学科要素，如规模法则、进化博弈论和发展模块性理论，建立了一种能够从静态数据中提取动态信息并重建移动网络的新框架。
Tiago\upcite{peixoto2019network}提出了一种可扩展的非参数贝叶斯方法，能够基于观察到的功能性行为进行网络重构，并同时推断网络中的社区结构，实现了重构与社区检测的协同效应。
Newman\upcite{newman2018network}提出了一种从复杂、多模态且带有噪声的数据中优化估计网络结构的新方法，适用于多种类型的数据格式，包括重复、矛盾、带有注释或缺失数据的情况，并在两个不同的社会网络中进行了应用。

%%% ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
